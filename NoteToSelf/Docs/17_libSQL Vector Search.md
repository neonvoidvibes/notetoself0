# libSQL Vector Search on iOS (Offline Semantic Search in Swift)

## Overview: libSQL Vector Indexing on Mobile

**libSQL** is a fork of SQLite that adds **native vector indexing** for similarity search. It integrates the DiskANN approximate nearest neighbor (ANN) algorithm directly into the database engine ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=Vector%20search%20queries%20for%20huge,libSQL%20to%20meet%20these%20needs)) ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=DiskANN%20is%20a%20graph,remaining%20efficient%20in%20search%20operations)). DiskANN builds a graph-based index on disk, so only a small portion of the index stays in memory during searches ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=DiskANN%20is%20a%20graph,remaining%20efficient%20in%20search%20operations)). This design yields a **small memory footprint and optimized I/O**, which is critical for mobile apps ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=algorithms,delays%20as%20much%20as%20possible)). In practice, you can store high-dimensional text embeddings (e.g. 384 or 768 floats) in libSQL and perform fast semantic searches on-device without loading all data into RAM.

**Performance & Scalability:** Using DiskANN, libSQL’s vector index scales to large local datasets while maintaining high recall (accuracy) for nearest neighbor queries ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=DiskANN%20is%20a%20graph,remaining%20efficient%20in%20search%20operations)). Instead of scanning every row (which becomes too slow beyond a few thousand entries) ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=Although%20the%20last%20query%20returns,for%20medium%20and%20large%20datasets)), the ANN index visits only a subset of vectors, dramatically speeding up search. The index is updated automatically upon inserts, as libSQL hooks DiskANN into SQLite’s indexing mechanism ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=index%20in%20a%20column,a%20table%20with%20vector%20index)). Empirically, libSQL’s ANN search can retrieve results in milliseconds even with tens of thousands of embeddings on a mobile device. The **DiskANN (LM-DiskANN)** variant used in libSQL is optimized for disk storage: it preloads neighbor information in 4KB blocks to minimize disk reads, trading a bit of storage for speed ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=contains%20sufficiently%20close%20candidates%20while,usage%20for%20an%20increase%20in)). This yields fast lookups with minimal RAM usage, ideal for offline-first apps. For example, one team found their database grew from ~3.5 MB of raw embeddings to ~50 MB after enabling a DiskANN index (about 19 MB was the index) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=To%20make%20the%20algorithm%20effective%2C,size%20after%20migrating%20to%20LibSQL)) – a significant space overhead – but queries became much faster. You can tune index parameters (discussed below) to balance memory, storage, and accuracy. By default the index uses **cosine distance** (1 – cosine similarity) as the metric and stores roughly `3 * √D` neighbors per vector (where D is the embedding dimension) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=This%20uses%2032,%3D%20around%2070%20neighbors)) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Default%3A%20,neighbors%20for%20every%20node%20in)). LibSQL supports up to **65,536-dimensional** vectors, which is far above typical embedding sizes ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,Indexing%20section%20for%20more%20information)). It also offers multiple storage formats for vectors – 64-bit, 32-bit, 16-bit floats, 8-bit fixed-point, and even a 1-bit binary representation – allowing you to trade-off precision vs. size ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,1%20%E2%80%94%20Cosine%20Similarity)) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=%60FLOAT8%60%20,achieving%20a%20very%20compact%20representation)). In most cases, 32-bit floats (FLOAT32) are a good default, but for large on-device datasets you can use compressed formats (e.g. 8-bit) to reduce storage at some cost to accuracy ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=While%201,well%20with%20our%20existing%20model)). Overall, libSQL’s built-in vector index brings **efficient semantic search** to mobile: you get sub-linear search speed, small memory usage, and the convenience of an SQLite-compatible database file on iOS.

## Step-by-Step Integration in a Swift iOS Project

To use libSQL for offline vector search in a Swift app, you’ll leverage the official **libSQL Swift SDK** (`libsql-swift`). Below are the steps to set up libSQL, store text embeddings, index them, and run top-**K** similarity queries – all on-device.

1. **Install libSQL via Swift Package Manager (SPM):** In Xcode, add the libSQL Swift package dependency. You can do this by editing your `Package.swift` or via Xcode’s Swift Packages interface. Point it to the GitHub repo `tursodatabase/libsql-swift` (version 0.1.1 or latest). For example, in `Package.swift`:

   ```swift
   dependencies: [
       .package(url: "https://github.com/tursodatabase/libsql-swift", from: "0.1.1")
   ],
   targets: [
       .target(name: "YourApp", dependencies: [.product(name: "Libsql", package: "libsql-swift")])
   ]
   ``` 

   This pulls in the libSQL library and Swift bindings. **Note:** The libSQL SDK supports iOS, macOS, tvOS, watchOS, etc. ([GitHub - tursodatabase/libsql-swift: libSQL bindings for Swift](https://github.com/tursodatabase/libsql-swift#:~:text=,iPadOS%2C%20tvOS%2C%20watchOS%20%26%20iOS)) ([GitHub - tursodatabase/libsql-swift: libSQL bindings for Swift](https://github.com/tursodatabase/libsql-swift#:~:text=,iPadOS%2C%20tvOS%2C%20watchOS%20%26%20iOS)). After adding the package, import the module in your Swift code with: 

   ```swift
   import Libsql
   ```

2. **Initialize a Local Database:** Choose a location for the database file on device (e.g. the app’s Documents directory). Create or open the SQLite database using the `Database` class. For an *offline-first* app, you can omit any remote URL to use the database purely locally:

   ```swift
   // Determine a file path for the database, e.g., Documents/embeddings.db
   let docsURL = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!
   let dbPath = docsURL.appendingPathComponent("embeddings.db").path

   // Open or create the local database
   let db = try Database(path: dbPath)
   let conn = try db.connect()
   ```
   
   The `Database` object manages the file, and `connect()` gives you a connection for executing SQL. (If you later want to sync with a remote Turso database, you could also supply a `url` and `authToken` here, but for offline use those aren’t needed ([Turso Quickstart (Swift) - Turso](https://docs.turso.tech/sdk/swift/quickstart#:~:text=Local%20only)).)

3. **Define a Table for Text Embeddings:** In the database, create a table to store your journal entries or chat messages along with their vector embeddings. Use one of libSQL’s **vector types** for the embedding column, specifying the dimensionality. For example, to store 512-dimensional float32 vectors:

   ```swift
   try conn.execute("""
       CREATE TABLE IF NOT EXISTS Entries (
           id        INTEGER PRIMARY KEY,    -- row identifier
           content   TEXT,                   -- the journal entry or message text
           embedding F32_BLOB(512)           -- 512-dim float32 embedding vector
       );
   """)
   ```
   
   Here `F32_BLOB(512)` denotes a blob of 512 32-bit floats ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=CREATE%20TABLE%20movies%20,dimensional%20f32%20vector)) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=%60FLOAT32%60%20,it%20using%20simple%20transformation%3A%20shift%2Balpha%E2%8B%85b)). (The number in parentheses is required and ensures each vector has exactly that many components.) LibSQL also accepts `FLOAT32` as a synonym ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Function%20name%20Description%20%60vector64%60%20,32%20cosine)), or other types like `F16_BLOB`, `F8_BLOB`, etc., if you prefer 16-bit or 8-bit compression for the vectors ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,1%20%E2%80%94%20Cosine%20Similarity)). The rest of the table can include any other fields you need (timestamps, metadata, etc.). 

4. **Insert Embeddings into the Table:** After you generate an embedding vector for a piece of text (using your ML model or API), insert it into the database. LibSQL provides a `vector` (or `vector32`) function to convert a JSON array representation into the binary blob format expected by the column ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=INSERT%20INTO%20movies%20,0.406%2C%200.027%2C%200.378%2C%200.056)). You can use parameterized queries to avoid manual string building. For example:

   ```swift
   // Example content and its embedding vector (replace with real data)
   let entryText = "Today I went to the park and enjoyed the sunshine."
   let embeddingVector: [Float] = ...  // 512-dim embedding from your model

   // Convert the Swift [Float] into a JSON array string like "[0.123, 0.456, ...]"
   let embJSON = "[" + embeddingVector.map(String.init).joined(separator: ", ") + "]"

   // Insert the new entry with its embedding
   try conn.execute(
       "INSERT INTO Entries (content, embedding) VALUES (?, vector32(?))",
       entryText, embJSON
   )
   ```
   
   In this code, `vector32(?)` takes the JSON string of the array and converts it to a binary FLOAT32 blob ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=INSERT%20INTO%20movies%20,0.406%2C%200.027%2C%200.378%2C%200.056)). We pass `entryText` and the `embJSON` string as parameters (`?` placeholders). The libSQL Swift SDK will bind these properly (text for content, text for the vector function). You could also bind the embedding as a raw `Data` blob if you pack the bytes yourself, but using `vector32(...)` is convenient. Repeat this insertion for each journal entry or chat message’s embedding. It’s efficient to do inserts in a batch or transaction if you have many (to reduce I/O overhead). The vector index (next step) can be created after all inserts, or even before – the index will update on each insert automatically ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=index%20in%20a%20column,a%20table%20with%20vector%20index)).

5. **Create a Vector Index on the Embedding Column:** To enable fast ANN search, create a special index using the `libsql_vector_idx()` function. This builds the DiskANN index for that vector column. For example:

   ```swift
   try conn.execute("""
       CREATE INDEX IF NOT EXISTS entries_embedding_idx
       ON Entries( libsql_vector_idx(embedding) );
   """)
   ```
   
   This instructs libSQL to build an ANN index (DiskANN graph) for the `embedding` column ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,or%20with%20singular%20PRIMARY%20KEY)). You can specify optional parameters inside `libsql_vector_idx` – such as a different distance metric or compression – but by default it uses cosine distance and the same precision as your column ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Setting%20key%20Value%20type%20Description,use%20for%20building%20the%20index)) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=%60compress_neighbors%20%60%60float1bit%60,in%20exchange%20to%20lower%20search)). For instance, you might do:
   
   ```sql
   CREATE INDEX entries_embedding_idx 
   ON Entries( libsql_vector_idx(embedding, 'metric=l2', 'compress_neighbors=float8') );
   ``` 
   
   to use Euclidean distance and compress neighbor vectors to 8-bit in the index ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=CREATE%20INDEX%20movies_idx%20ON%20movies,metric%3Dl2%27%2C%20%27compress_neighbors%3Dfloat8)). Tuning these is discussed under **Best Practices**. Index creation may take some time if you have a lot of embeddings (it’s building the graph), but it’s a one-time cost. Once created, any new insertions will update the index incrementally. The index data is stored in the database file (e.g. as shadow tables), which will increase the file size ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=To%20make%20the%20algorithm%20effective%2C,size%20after%20migrating%20to%20LibSQL)).

6. **Perform Top-K Semantic Search Queries:** With data indexed, you can query for the nearest neighbors to a given embedding – for example, to find similar journal entries to a new entry or to a search query’s embedding. libSQL provides a **table-valued function** `vector_top_k(index_name, query_vector, K)` that returns the top *K* closest matches from the index ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=To%20search%20the%20vector%20index%2C,you%20would%20do%20the%20following)) ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,or%20with%20singular%20PRIMARY%20KEY)). You use this in a SQL query (often with a JOIN) to retrieve the actual rows. In Swift, you might do:

   ```swift
   // Suppose we have a query vector we want to search for:
   let queryVector: [Float] = ...  // e.g., embedding of search text
   let queryJSON = "[" + queryVector.map(String.init).joined(separator: ", ") + "]"
   let topK = 5

   // Execute a nearest-neighbors query using the vector index:
   let rows = try conn.query("""
       SELECT E.id, E.content
       FROM Entries AS E
       JOIN vector_top_k('entries_embedding_idx', vector32(?), ?) AS V
         ON E.rowid = V.id
       """, queryJSON, topK)

   for row in rows {
       // Each row corresponds to one of the top-K nearest entries
       let resultId: Int64 = row["id"] ?? 0
       let resultText: String = row["content"] ?? ""
       print("Result \(resultId): \(resultText)")
   }
   ```
   
   In this SQL, `vector_top_k('entries_embedding_idx', vector32(?), ?)` returns a transient table of the `K` nearest neighbors’ row IDs (and their distances) from the ANN index ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,or%20with%20singular%20PRIMARY%20KEY)). We join that with the `Entries` table on `rowid` (SQLite’s implicit primary key) to get the full content. The result is the top 5 most similar entries to the `queryVector`. Alternatively, you can use the index in a `WHERE ... IN (...)` subquery style as shorthand ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=,you%20would%20do%20the%20following)):
   
   ```sql
   SELECT content 
   FROM Entries 
   WHERE id IN vector_top_k('entries_embedding_idx', vector32('[...query vector... ]'), 5);
   ``` 
   
   Both approaches consult the ANN index. Remember that using the index is *not automatic* – you **must** use `vector_top_k` in the query to trigger the indexed search ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=Considering%20Vector%20Search%20uses%20larger,function)) ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,or%20with%20singular%20PRIMARY%20KEY)). (If you simply used `ORDER BY vector_distance_cos(embedding, vector32(?)) LIMIT 5` without the index, SQLite would fallback to a full scan, which is slow for large data.) With the index, the search will be fast even as your local dataset grows to thousands of embeddings. The results can then be used in your Swift UI, for example, to display similar past entries to the user’s current note, or to provide semantic search results from their history.

## Best Practices for Performance on iOS

Storing and querying vectors on a mobile device requires careful optimization. Here are some best practices to ensure efficient performance and low memory usage:

- **Choose the Right Vector Precision:** Use the **lowest precision** that still preserves accuracy for your use case. By default, 32-bit floats are used, but libSQL supports compressed formats like 16-bit or 8-bit floats, and even 1-bit encodings ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,1%20%E2%80%94%20Cosine%20Similarity)) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=While%201,well%20with%20our%20existing%20model)). For example, using `F8_BLOB` (8-bit) for your `embedding` column or specifying `'compress_neighbors=float8'` in the index can drastically reduce storage size (the index can shrink ~3× with float8 compression, and up to 8× when combined with fewer neighbors) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=Then%2C%20I%20ran%20a%20size,8x%20improvement%3A%2019705856%20%2F%202404352)). This comes with a slight loss in recall/precision of similarity matching. Test with your data: if search quality remains acceptable, a smaller vector type will save disk space and I/O. (Note: **1-bit** vectors yield the smallest size, but they require a special embedding model trained for binary vectors ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=CREATE%20INDEX%20movies_idx%20ON%20movies,embedding%2C%20%27metric%3Dhamming%27%29)) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=While%201,well%20with%20our%20existing%20model)), so they are advanced usage.)

- **Tune Index Parameters:** LibSQL allows tuning the DiskANN index to balance speed, memory, and accuracy ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Setting%20key%20Value%20type%20Description,use%20for%20building%20the%20index)) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=,200)). Important settings include:
  - **`max_neighbors`**: The number of neighbors stored for each vector in the graph (default ~`3 * √D` per vector). Lowering this will make the index file smaller but can degrade search recall ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Default%3A%20,for%20neighbors%20%E2%80%94%20the%20less)). If your embeddings are high-dimensional (e.g. 512), the default neighbors count (~70) might be overkill; you could try a smaller number (e.g. 20–50) to save space ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=My%20main%20concern%20was%20search,neighbors%20compare%20to%2070%20neighbors)). In one case, reducing from ~70 to 20 neighbors cut index size 8× with minimal impact on result quality ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=Then%2C%20I%20ran%20a%20size,8x%20improvement%3A%2019705856%20%2F%202404352)).
  - **`metric`**: The distance metric used for ANN. **Cosine** is default and suitable for text embeddings (which are often cosine-normalized) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Setting%20key%20Value%20type%20Description,use%20for%20building%20the%20index)). If your embeddings work better with Euclidean distance, set `'metric=l2'` when creating the index ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=CREATE%20INDEX%20movies_idx%20ON%20movies,metric%3Dl2%27%2C%20%27compress_neighbors%3Dfloat8)). Ensure you use the corresponding distance function (e.g. `vector_distance_l2`) if doing any explicit distance calculations.
  - **`compress_neighbors`**: The precision for storing neighbor vectors in the index graph. By default it uses the same type as your base data (no compression), but you can set a smaller type here just for the index to save space ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Default%3A%203%20D%20where%20D,%E2%80%94%20the%20more%20sparse%20is)). For example, you can keep your main `embedding` as F32 for maximum accuracy in other uses, but use `'compress_neighbors=float8'` in the index to compress those copies of vectors in the DiskANN graph ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=CREATE%20INDEX%20movies_idx%20ON%20movies,metric%3Dl2%27%2C%20%27compress_neighbors%3Dfloat8)). This can significantly reduce index size with minor impact on ANN accuracy ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=While%201,well%20with%20our%20existing%20model)) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=Then%2C%20I%20ran%20a%20size,8x%20improvement%3A%2019705856%20%2F%202404352)).
  - **`search_l`** and **`alpha`**: These control the ANN search effort. `search_l` (default 200) is the candidate pool size during search; lowering it speeds up queries but may drop recall ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Default%3A%20no%20compression%20,in%20exchange%20to%20search%20precision)). `alpha` (density factor, default 1.2) controls graph sparsity ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=storage%20index%20will%20use%20in,the%20amount%20of%20neighbors%20visited)). Advanced users can experiment with these if needed – e.g., on slower devices you might lower `search_l` for faster but slightly less-accurate results, or reduce `alpha` to prune the graph for speed. The defaults are generally reasonable for a good trade-off, so tune only if you observe performance issues.
  - **Batch Index Building:** If you plan to load a large number of embeddings at once (say on first app launch), you might consider creating the index **after** inserting them all (or drop and recreate it), rather than updating it on every insert. Bulk index construction can be more I/O-efficient. However, libSQL’s integration will handle incremental inserts fine; just be aware that a huge one-time bulk insert will also spend time updating the index for each row. You can wrap mass inserts in a transaction to speed up the SQLite side, and the DiskANN updates will batch at commit.

- **Query Efficiency and Batching:** For performing multiple searches, reuse the database connection to avoid re-initializing SQLite each time. You can prepare the `SELECT ... JOIN vector_top_k(...)` statement once and bind different query vectors to it, rather than constructing a new SQL string every time. This reduces parsing overhead. If you have many query vectors to run (e.g., in a background task), you could execute them in a single transaction or even in parallel on separate connections (SQLite allows concurrent reads). Typically, though, semantic searches are user-driven and not needed in bulk. The `vector_top_k` already gives you the top-K in one call, so you don’t need to manually filter or post-process many results. Just ensure you limit K to what you truly need (e.g. top 5 or 10) to keep the query fast. 

- **Memory Management:** Because libSQL (SQLite) operates within your app’s process, be mindful of memory. The **LM-DiskANN** index is designed to be memory-light – it only loads small chunks of the graph as needed ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=variant%20of%20the%20algorithm%20we,remaining%20efficient%20in%20search%20operations)) ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=difference%20lies%20between%20original%20DiskANN,for%20an%20increase%20in%20storage)). Still, avoid loading large query result sets entirely into memory. For example, if you expect thousands of neighbors (which is uncommon for a user-facing query), use a reasonable LIMIT. Also, closing the `Connection` when not in use (or using a single shared connection) can help SQLite reuse memory effectively. The database file access will use the OS disk cache; on iOS devices with fast flash storage, sequential reads of the DiskANN blocks are quite efficient. Ensure that the device has storage space for the index – as noted, the index can be much larger than the raw data ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=increase%20in%20database%20size%20after,migrating%20to%20LibSQL)). If space is an issue, leverage the compression techniques above or limit the amount of data you keep on device (perhaps by pruning old entries or capping the dataset size per user).

- **Testing and Profiling:** Because every use case is different, profile your search latency and memory on a real device. libSQL’s vector search has been used in production mobile apps (e.g. the *Kin* personal AI app) to enable on-device retrieval augmented generation ([Building Vector Search and Personal Knowledge Graphs on Mobile with libSQL and React Native](https://turso.tech/blog/building-vector-search-and-personal-knowledge-graphs-on-mobile-with-libsql-and-react-native#:~:text=At%20Kin%2C%20we%20develop%20AI,on%20both%20iOS%20and%20Android)) ([Building Vector Search and Personal Knowledge Graphs on Mobile with libSQL and React Native](https://turso.tech/blog/building-vector-search-and-personal-knowledge-graphs-on-mobile-with-libsql-and-react-native#:~:text=This%20means%20that%20complex%20RAG,occur%20on%20the%20user%27s%20device)). You should expect that queries using the ANN index remain responsive (tens of milliseconds) even as your embedding count grows, whereas a brute-force search would degrade linearly. If you notice any slow queries, ensure the `vector_top_k` index is being used (you can prefix the query with `EXPLAIN QUERY PLAN` to confirm it’s using the vector index). Also consider the effect of embedding dimensionality on both memory and speed – higher dimensions make distance calculations heavier and increase index size. If using very high-dimensional embeddings (hundreds), you might explore reducing the dimension (via PCA or using a smaller model) for a leaner index on mobile.

## Licensing and App Store Considerations

**libSQL is open-source (MIT licensed)** ([libsql - npm](https://www.npmjs.com/package/libsql/v/0.1.13?activeTab=readme#:~:text=libsql%20,MIT%2C%20without%20any%20additional)), which makes it suitable for commercial use. You can integrate it into an App Store application with no license fees or copyleft concerns. The MIT license is permissive; just include an attribution in your app’s acknowledgments as needed. Apple has no restrictions against SQLite or similar libraries – in fact, libSQL is a drop-in replacement for SQLite, which is commonly used in iOS apps. The libSQL Swift SDK itself is also MIT licensed (inherited from the core project), so you can freely use and distribute it within your app ([GitHub - tursodatabase/libsql-swift: libSQL bindings for Swift](https://github.com/tursodatabase/libsql-swift#:~:text=)) ([GitHub - tursodatabase/libsql-swift: libSQL bindings for Swift](https://github.com/tursodatabase/libsql-swift#:~:text=Readme)). 

One consideration: if you enable **encryption** (libSQL supports encryption at rest, unlike stock SQLite), be aware of Apple’s export compliance rules for cryptography. Otherwise, using libSQL for offline data and AI features is straightforward. There are no special App Store constraints beyond what standard SQLite usage would entail. In summary, libSQL’s licensing permits integration into any commercial app, and its single-file, embeddable database is easy to bundle with your iOS project. This means you can ship your offline-first semantic search feature confidently, knowing that the data stays on-device and under the user’s control ([Building Vector Search and Personal Knowledge Graphs on Mobile with libSQL and React Native](https://turso.tech/blog/building-vector-search-and-personal-knowledge-graphs-on-mobile-with-libsql-and-react-native#:~:text=All%20of%20these%20principles%20have,the%20primary%20processing%20happens%20locally)), and that you’re abiding by licensing requirements.

**Sources:**

- Pekka Enberg and Nikita Sivukhin, *“Approximate nearest neighbor search with DiskANN in libSQL,”* Turso Tech Blog, Jul. 2024. ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=Vector%20search%20queries%20for%20huge,libSQL%20to%20meet%20these%20needs)) ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=DiskANN%20is%20a%20graph,remaining%20efficient%20in%20search%20operations)) ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=index%20in%20a%20column,a%20table%20with%20vector%20index)) ([Approximate nearest neighbor search with DiskANN in libSQL](https://turso.tech/blog/approximate-nearest-neighbor-search-with-diskann-in-libsql#:~:text=To%20search%20the%20vector%20index%2C,you%20would%20do%20the%20following))

- Volodymyr Pavlyshyn, *“The space complexity of vector indexes in LibSQL,”* Turso Tech Blog, Oct. 2024. ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=To%20make%20the%20algorithm%20effective%2C,size%20after%20migrating%20to%20LibSQL)) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=algorithms,delays%20as%20much%20as%20possible)) ([The space complexity of vector indexes in LibSQL](https://turso.tech/blog/the-space-complexity-of-vector-indexes-in-libsql#:~:text=Then%2C%20I%20ran%20a%20size,8x%20improvement%3A%2019705856%20%2F%202404352))

- Azion Docs, *“Edge SQL Vector Search,”* (using libSQL under the hood) ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,1%20%E2%80%94%20Cosine%20Similarity)) ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,Indexing%20section%20for%20more%20information)) ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=,or%20with%20singular%20PRIMARY%20KEY)) ([Edge SQL Vector Search - Azion Documentation](https://www.azion.com/en/documentation/products/store/edge-sql/vector-search/#:~:text=Considering%20Vector%20Search%20uses%20larger,function))

- Turso Documentation, *“AI & Embeddings - How it works,”* which covers libSQL vector functions and index settings ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=CREATE%20TABLE%20movies%20,dimensional%20f32%20vector)) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=Default%3A%20,for%20neighbors%20%E2%80%94%20the%20less)) ([AI & Embeddings - Turso](https://docs.turso.tech/features/ai-and-embeddings#:~:text=storage%20index%20will%20use%20in,in%20exchange%20to%20search%20precision)).

- GitHub – libSQL Swift SDK README and Turso Swift Quickstart ([GitHub - tursodatabase/libsql-swift: libSQL bindings for Swift](https://github.com/tursodatabase/libsql-swift#:~:text=,iPadOS%2C%20tvOS%2C%20watchOS%20%26%20iOS)) ([Turso Quickstart (Swift) - Turso](https://docs.turso.tech/sdk/swift/quickstart#:~:text=Local%20only)) (setup and usage in Swift).

- *Kin* Case Study – *“Building Vector Search on Mobile with libSQL,”* Turso Blog, Oct. 2024 (real-world usage on iOS) ([Building Vector Search and Personal Knowledge Graphs on Mobile with libSQL and React Native](https://turso.tech/blog/building-vector-search-and-personal-knowledge-graphs-on-mobile-with-libsql-and-react-native#:~:text=This%20means%20that%20complex%20RAG,occur%20on%20the%20user%27s%20device)).

